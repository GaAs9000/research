"""
BC-GNN é¢„æµ‹å™¨ï¼ˆV-Î¸ è·¯çº¿ï¼‰ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¾“å‡ºè¾¹ç•ŒåŠŸç‡ä¸æ¯çº¿æ³¨å…¥ã€‚
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Optional
import sys
import os

# å¯¼å…¥BC-GNNæ¨¡å‹ç»“æ„ï¼ˆä½¿ç”¨ä»“åº“å†…å®ç°ï¼‰
from gnn.model import BCGNN


class BCGNNPredictor:
    """BC-GNNé¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str, device: str = 'auto'):
        """
        åˆå§‹åŒ–BC-GNNé¢„æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„BC-GNNæ¨¡å‹æƒé‡è·¯å¾„
            device: æ¨ç†è®¾å¤‡ ('auto', 'cpu', 'cuda')
        """
        self.model_path = model_path
        self.device = self._get_device(device)
        self.model = None
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _get_device(self, device: str) -> torch.device:
        """è·å–æ¨ç†è®¾å¤‡"""
        if device == 'auto':
            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            return torch.device(device)
    
    def _load_model(self) -> None:
        """åŠ è½½BC-GNNæ¨¡å‹"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
        
        try:
            # åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
            checkpoint = torch.load(self.model_path, map_location=self.device)
            
            # æå–æ¨¡å‹çŠ¶æ€å’Œé…ç½®
            if 'model_state_dict' in checkpoint:
                model_state = checkpoint['model_state_dict']
                # å°è¯•ä»é…ç½®ä¸­è¯»å–æ¨¡å‹å‚æ•°
                config_obj = checkpoint.get('config', {})
                # å¦‚æœconfigæ˜¯å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
                if hasattr(config_obj, '__dict__'):
                    config = config_obj.__dict__
                elif isinstance(config_obj, dict):
                    config = config_obj
                else:
                    config = {}
            else:
                # å…¼å®¹ç›´æ¥ä¿å­˜æ¨¡å‹çŠ¶æ€çš„æƒ…å†µ
                model_state = checkpoint
                config = {}
            
            # ä»æ¨¡å‹çŠ¶æ€ä¸­æ¨æ–­æ¨¡å‹é…ç½®
            model_config = self._infer_model_config(model_state, config)
            
            # å®ä¾‹åŒ–æ¨¡å‹
            self.model = BCGNN(**model_config)
            
            # åŠ è½½æƒé‡
            self.model.load_state_dict(model_state)
            self.model.to(self.device)
            self.model.eval()
            
            print(f"âœ… æˆåŠŸåŠ è½½BC-GNNæ¨¡å‹: {self.model_path}")
            print(f"ğŸ“ æ¨ç†è®¾å¤‡: {self.device}")
            print(f"âš™ï¸  æ¨¡å‹é…ç½®: {model_config}")
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½BC-GNNæ¨¡å‹å¤±è´¥: {e}")
    
    def _infer_model_config(self, model_state: Dict, config: Dict) -> Dict:
        """ä»æ¨¡å‹çŠ¶æ€æ¨æ–­æ¨¡å‹é…ç½®"""
        model_config = {}
        
        # ä»state_dictæ¨æ–­å‚æ•°
        if 'node_encoder.weight' in model_state:
            node_features = model_state['node_encoder.weight'].shape[1]
            hidden_dim = model_state['node_encoder.weight'].shape[0]
            model_config['node_features'] = node_features
            model_config['hidden_dim'] = hidden_dim
        
        # ä»é…ç½®æˆ–é»˜è®¤å€¼è®¾ç½®å…¶ä»–å‚æ•°
        # V1.1 edge_attr: 10 ç»´
        model_config['edge_features'] = config.get('edge_features', 10)
        model_config['n_mpnn_layers'] = config.get('n_mpnn_layers', 4)
        model_config['jk_mode'] = config.get('jk_mode', 'cat')
        model_config['use_voltage_prior'] = config.get('use_voltage_prior', True)
        model_config['use_global'] = config.get('use_global', False)
        
        # ç”µå‹ç»Ÿè®¡é‡
        voltage_stats = config.get('voltage_stats')
        if voltage_stats:
            model_config['voltage_stats'] = voltage_stats
        
        return model_config
    
    def predict(self, pyg_data: Any) -> Dict:
        """
        å¯¹PyGæ•°æ®è¿›è¡ŒBC-GNNé¢„æµ‹
        
        Args:
            pyg_data: PyG Dataå¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´çš„å›¾ç»“æ„å’Œç‰¹å¾
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸ï¼š
            {
                'success': bool,
                'boundary_pq': {bus_id: (P_bd, Q_bd)},
                'corridor_ports': {(u,v): (Pu, Pv, Qu, Qv)},
                'error': é”™è¯¯ä¿¡æ¯ (å¦‚æœå¤±è´¥)
            }
        """
        if self.model is None:
            return {
                'success': False,
                'error': "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨_load_model()"
            }
        
        try:
            # å°†æ•°æ®ç§»åŠ¨åˆ°æ¨ç†è®¾å¤‡
            pyg_data = pyg_data.to(self.device)
            
            # å‰å‘æ¨ç†
            with torch.no_grad():
                predictions = self.model(pyg_data)
            
            # ç«¯å£çº§é¢„æµ‹
            if 'corridor_pfqt' not in predictions:
                raise RuntimeError('æ¨¡å‹æœªè¾“å‡º corridor_pfqtï¼ˆç«¯å£çº§ [pf_u,pt_v,qf_u,qt_v]ï¼‰')
            end_ports = predictions['corridor_pfqt']  # [C,4]
            # å°†ç«¯å£æ˜ å°„ä¸ºæ¯çº¿è¾¹ç•Œæ³¨å…¥ï¼ˆæŒ‰ tie_buses é¡ºåºï¼‰ï¼šuç«¯ -Puï¼Œvç«¯ +Pv
            if not (hasattr(pyg_data, 'tie_corridors') and hasattr(pyg_data, 'tie_buses')):
                raise RuntimeError('ç¼ºå°‘ tie_corridors / tie_buses å­—æ®µï¼Œæ— æ³•åšæ¯çº¿æ˜ å°„')
            device = end_ports.device
            tie_corr = pyg_data.tie_corridors.to(device)
            tie_buses = pyg_data.tie_buses.to(device)
            B = tie_buses.numel()
            bus2idx = torch.full((int(tie_buses.max().item()) + 1,), -1, dtype=torch.long, device=device)
            bus2idx[tie_buses.long()] = torch.arange(B, device=device)
            agg = torch.zeros(B, 2, dtype=torch.float32, device=device)
            u = tie_corr[:, 0].long(); v = tie_corr[:, 1].long()
            Pu, Pv = end_ports[:, 0], end_ports[:, 1]
            Qu, Qv = end_ports[:, 2], end_ports[:, 3]
            idx_u = bus2idx[u]
            idx_v = bus2idx[v]
            agg.index_add_(0, idx_u, torch.stack([-Pu, -Qu], dim=-1))
            agg.index_add_(0, idx_v, torch.stack([+Pv, +Qv], dim=-1))
            bus_pq_pred = agg  # [B,2]
            # æ‰“åŒ…ä¸ºå­—å…¸ï¼ˆbus_id -> (P,Q)ï¼‰
            boundary_pq = {}
            buses = pyg_data.tie_buses.cpu().tolist()
            for i, b in enumerate(buses):
                P, Q = bus_pq_pred[i].detach().cpu().tolist()
                boundary_pq[int(b)] = (float(P), float(Q))

            return {
                'success': True,
                'boundary_pq': boundary_pq,
                'corridor_pfqt': end_ports.detach().cpu().numpy().tolist(),
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f"BC-GNNé¢„æµ‹å¤±è´¥: {e}"
            }
    
    def predict_batch(self, pyg_data_list) -> list:
        """
        æ‰¹é‡é¢„æµ‹ï¼ˆæš‚æ—¶ç®€å•å¾ªç¯ï¼Œæœªæ¥å¯ä¼˜åŒ–ä¸ºçœŸæ­£çš„batchæ¨ç†ï¼‰
        
        Args:
            pyg_data_list: PyG Dataå¯¹è±¡åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for pyg_data in pyg_data_list:
            result = self.predict(pyg_data)
            results.append(result)
        return results
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return {}
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'model_path': self.model_path,
            'device': str(self.device),
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,  # å‡è®¾float32
        }
