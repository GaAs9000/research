"""
OPFData å¤„ç†ä¼˜åŒ–

æœ¬æ¨¡å—æä¾›äº†ä¼˜åŒ–çš„æ•°æ®å¤„ç†å’Œå­˜å‚¨æ–¹æ¡ˆï¼Œ
æ—¨åœ¨æå‡ GPU åŠ é€Ÿæ•ˆæœå¹¶å‡å°‘æ•°æ®åŠ è½½æ—¶é—´ã€‚
"""

import torch
import pickle
import h5py
import numpy as np
from typing import List, Dict
from torch_geometric.data import Data, Batch
import os
import time


class OptimizedProcessor:
    """
    ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„æ•°æ®å¤„ç†å™¨ï¼Œæ”¯æŒ GPU åŠ é€Ÿå’Œå¿«é€Ÿ I/Oã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨ï¼Œå¹¶æ£€æµ‹å¯ç”¨çš„è®¡ç®—è®¾å¤‡ï¼ˆCUDA GPU æˆ– CPUï¼‰ã€‚"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä¼˜åŒ–å¤„ç†å™¨ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def save_batch_optimized(self, samples: List[Data], output_path: str, format='pt'):
        """
        ä»¥ä¼˜åŒ–çš„æ ¼å¼ä¿å­˜ä¸€æ‰¹æ ·æœ¬ã€‚

        æ”¯æŒçš„æ ¼å¼ï¼š
        - 'pkl': Pickle æ ¼å¼ï¼Œåºåˆ—åŒ– Python å¯¹è±¡ï¼Œé€‚åˆå°æ•°æ®é›†ï¼Œé€Ÿåº¦å¿«ã€‚
        - 'h5': HDF5 æ ¼å¼ï¼Œåˆ†å±‚æ•°æ®æ ¼å¼ï¼Œé€‚åˆå¤§æ•°æ®é›†ï¼Œæ”¯æŒå‹ç¼©ã€‚
        - 'pt': PyTorch åŸç”Ÿæ ¼å¼ï¼Œç›´æ¥ä¿å­˜ Tensor å¯¹è±¡ã€‚

        Args:
            samples (List[Data]): ä¸€ä¸ªåŒ…å« PyG Data å¯¹è±¡çš„åˆ—è¡¨ã€‚
            output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
            format (str): ä¿å­˜æ ¼å¼ã€‚
        """
        if format == 'pkl':
            with open(output_path, 'wb') as f:
                pickle.dump(samples, f, protocol=pickle.HIGHEST_PROTOCOL)
                
        elif format == 'h5':
            with h5py.File(output_path, 'w') as f:
                for i, sample in enumerate(samples):
                    group = f.create_group(f'sample_{i}')
                    # å°†æ¯ä¸ª tensor è½¬æ¢ä¸º numpy array å¹¶ä½¿ç”¨ gzip å‹ç¼©å­˜å‚¨
                    group.create_dataset('x', data=sample.x.cpu().numpy(), compression='gzip')
                    group.create_dataset('edge_index', data=sample.edge_index.cpu().numpy(), compression='gzip')
                    group.create_dataset('edge_attr', data=sample.edge_attr.cpu().numpy(), compression='gzip')
                    group.create_dataset('y', data=sample.y.cpu().numpy(), compression='gzip')
                    group.create_dataset('tie_buses', data=sample.tie_buses.cpu().numpy(), compression='gzip')
                    group.create_dataset('tie_lines', data=sample.tie_lines.cpu().numpy(), compression='gzip')
                    
        elif format == 'pt':
            # ç›´æ¥ä½¿ç”¨ PyTorch çš„ save å‡½æ•°
            torch.save(samples, output_path)
    
    def load_batch_optimized(self, file_path: str, format='pkl', to_gpu=False) -> List[Data]:
        """
        ä»ä¼˜åŒ–æ ¼å¼çš„æ–‡ä»¶ä¸­åŠ è½½ä¸€æ‰¹æ ·æœ¬ã€‚

        Args:
            file_path (str): è¾“å…¥æ–‡ä»¶è·¯å¾„ã€‚
            format (str): æ–‡ä»¶æ ¼å¼ ('pkl', 'h5', 'pt')ã€‚
            to_gpu (bool): æ˜¯å¦åœ¨åŠ è½½åç«‹å³å°†æ•°æ®ç§»åŠ¨åˆ° GPUã€‚

        Returns:
            List[Data]: ä¸€ä¸ªåŒ…å«åŠ è½½çš„ PyG Data å¯¹è±¡çš„åˆ—è¡¨ã€‚
        """
        if format == 'pkl':
            with open(file_path, 'rb') as f:
                samples = pickle.load(f)
                
        elif format == 'h5':
            samples = []
            with h5py.File(file_path, 'r') as f:
                for key in sorted(f.keys(), key=lambda x: int(x.split('_')[1])):
                    group = f[key]
                    sample = Data(
                        x=torch.tensor(group['x'][...], dtype=torch.float32),
                        edge_index=torch.tensor(group['edge_index'][...], dtype=torch.long),
                        edge_attr=torch.tensor(group['edge_attr'][...], dtype=torch.float32),
                        y=torch.tensor(group['y'][...], dtype=torch.float32),
                        tie_buses=torch.tensor(group['tie_buses'][...], dtype=torch.long),
                        tie_lines=torch.tensor(group['tie_lines'][...], dtype=torch.long)
                    )
                    samples.append(sample)
                    
        elif format == 'pt':
            # åŠ è½½ .pt æ–‡ä»¶æ—¶ï¼Œé»˜è®¤å…ˆæ˜ å°„åˆ° CPUï¼Œé¿å…ç›´æ¥åŠ è½½åˆ° GPU å¯¼è‡´æ˜¾å­˜é—®é¢˜
            samples = torch.load(file_path, map_location='cpu')
        
        # å¦‚æœéœ€è¦ï¼Œå°†åŠ è½½çš„æ ·æœ¬æ‰¹é‡ç§»åŠ¨åˆ° GPU
        if to_gpu and torch.cuda.is_available():
            samples = [sample.to(self.device) for sample in samples]
            
        return samples
    
    def create_gpu_batch(self, samples: List[Data]) -> Batch:
        """
        ä»æ ·æœ¬åˆ—è¡¨åˆ›å»ºä¸€ä¸ªåœ¨ GPU ä¸Šä¼˜åŒ–çš„ PyG Batch å¯¹è±¡ã€‚

        Args:
            samples (List[Data]): PyG Data å¯¹è±¡åˆ—è¡¨ã€‚

        Returns:
            Batch: ä¸€ä¸ªåœ¨ GPU ä¸Šçš„ PyG Batch å¯¹è±¡ã€‚
        """
        # å…ˆå°†æ‰€æœ‰æ ·æœ¬ç§»åŠ¨åˆ° GPU
        gpu_samples = [sample.to(self.device) for sample in samples]
        
        # ä½¿ç”¨ from_data_list æ–¹æ³•å°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªå¤§å›¾ (Batch)
        batch = Batch.from_data_list(gpu_samples)
        
        return batch
    
    def benchmark_formats(self, samples: List[Data], test_file: str = '/tmp/test_format'):
        """
        å¯¹ä¸åŒçš„å­˜å‚¨æ ¼å¼è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

        æµ‹è¯•æŒ‡æ ‡åŒ…æ‹¬ï¼šä¿å­˜æ—¶é—´ã€åŠ è½½æ—¶é—´ã€æ–‡ä»¶å¤§å°ã€åˆ° GPU çš„ä¼ è¾“æ—¶é—´ã€‚

        Args:
            samples (List[Data]): ç”¨äºæµ‹è¯•çš„æ ·æœ¬æ•°æ®ã€‚
            test_file (str): æµ‹è¯•æ–‡ä»¶çš„åŸºæœ¬è·¯å¾„ã€‚
        """
        formats = ['pkl', 'h5', 'pt']
        results = {}
        
        print("=== å­˜å‚¨æ ¼å¼åŸºå‡†æµ‹è¯• ===")
        
        for fmt in formats:
            file_path = f"{test_file}.{fmt}"
            
            # æµ‹è¯•ä¿å­˜æ—¶é—´
            start_time = time.time()
            self.save_batch_optimized(samples, file_path, format=fmt)
            save_time = time.time() - start_time
            
            # æµ‹è¯•æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # è½¬æ¢ä¸º MB
            
            # æµ‹è¯•åŠ è½½æ—¶é—´
            start_time = time.time()
            loaded_samples = self.load_batch_optimized(file_path, format=fmt)
            load_time = time.time() - start_time
            
            # æµ‹è¯•åˆ° GPU çš„ä¼ è¾“æ—¶é—´
            gpu_time = 0
            if torch.cuda.is_available():
                start_time = time.time()
                gpu_samples = [sample.to(self.device) for sample in loaded_samples]
                gpu_time = time.time() - start_time
            
            results[fmt] = {
                'save_time': save_time,
                'load_time': load_time,
                'gpu_time': gpu_time,
                'file_size': file_size
            }
            
            print(f"{fmt.upper()}: ä¿å­˜ {save_time:.3f}s, åŠ è½½ {load_time:.3f}s, "
                  f"GPUä¼ è¾“ {gpu_time:.3f}s, æ–‡ä»¶å¤§å° {file_size:.2f}MB")
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove(file_path)
        
        # æ¨èç»¼åˆæ€§èƒ½æœ€ä½³çš„æ ¼å¼
        best_overall = min(results.keys(), 
                           key=lambda x: results[x]['save_time'] + results[x]['load_time'])
        print(f"\nğŸ’¡ æ¨èæ ¼å¼: {best_overall.upper()} (ç»¼åˆè¯»å†™æ€§èƒ½æœ€ä½³)")
        
        return results


class DatasetOptimizer:
    """Optimize entire dataset processing and storage."""
    
    def __init__(self, chunk_size: int = 1000):
        self.chunk_size = chunk_size
        self.optimizer = OptimizedProcessor()
    
    def process_and_save_chunks(self, json_dir: str, output_dir: str, format: str = 'pkl'):
        """Process dataset in chunks to optimize memory usage.
        
        Args:
            json_dir: Directory with JSON files
            output_dir: Output directory for processed chunks
            format: Storage format ('pkl', 'h5', 'pt')
        """
        from opfdata.processor import OPFDataProcessor
        
        os.makedirs(output_dir, exist_ok=True)
        
        processor = OPFDataProcessor()
        json_files = sorted([f for f in os.listdir(json_dir) if f.endswith('.json')])
        
        print(f"Processing {len(json_files)} files in chunks of {self.chunk_size}")
        print(f"Output format: {format}")
        
        chunk_id = 0
        total_samples = 0
        
        for i in range(0, len(json_files), self.chunk_size):
            chunk_files = json_files[i:i + self.chunk_size]
            chunk_samples = []
            
            print(f"\nProcessing chunk {chunk_id}: files {i}-{min(i+self.chunk_size-1, len(json_files)-1)}")
            
            # Process files in current chunk
            for json_file in chunk_files:
                json_path = os.path.join(json_dir, json_file)
                try:
                    samples = processor.process_single_json(json_path)
                    chunk_samples.extend(samples)
                except Exception as e:
                    print(f"Failed to process {json_file}: {e}")
                    continue
            
            # Save chunk
            if chunk_samples:
                output_path = os.path.join(output_dir, f'chunk_{chunk_id:04d}.{format}')
                self.optimizer.save_batch_optimized(chunk_samples, output_path, format)
                
                total_samples += len(chunk_samples)
                print(f"Saved {len(chunk_samples)} samples to {output_path}")
            
            chunk_id += 1
        
        print(f"\nâœ… Processing complete: {total_samples} samples in {chunk_id} chunks")
        
        # Create metadata file
        metadata = {
            'total_samples': total_samples,
            'num_chunks': chunk_id,
            'chunk_size': self.chunk_size,
            'format': format,
            'files_processed': len(json_files)
        }
        
        metadata_path = os.path.join(output_dir, 'metadata.pkl')
        with open(metadata_path, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"Metadata saved to {metadata_path}")
        
        return metadata