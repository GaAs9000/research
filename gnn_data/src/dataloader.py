"""
è®­ç»ƒæ•°æ®åŠ è½½å™¨ - æ”¯æŒæŒ‰åˆ†åŒºæ•°åˆ†ç»„çš„æ™ºèƒ½batchç­–ç•¥
"""

import torch
from torch.utils.data import Dataset, DataLoader
from torch_geometric.data import Batch
from pathlib import Path
import random
from typing import List, Dict, Optional, Union
from collections import defaultdict

from opfdata.optimizer import OptimizedProcessor


class OPFDataset(Dataset):
    """
    OPFData è®­ç»ƒæ•°æ®é›†ã€‚
    
    è¯¥æ•°æ®é›†æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. group_by_k=True: æŒ‰åˆ†åŒºæ•° k å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç»„ã€‚è¿™åœ¨è®­ç»ƒæ—¶éå¸¸æœ‰ç”¨ï¼Œ
       å¯ä»¥ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡éƒ½åŒ…å«æ¥è‡ªä¸åŒ k å€¼çš„æ ·æœ¬ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
    2. group_by_k=False: å°†æ‰€æœ‰æ ·æœ¬è§†ä¸ºä¸€ä¸ªæ‰å¹³åˆ—è¡¨ï¼Œè¿›è¡Œå¸¸è§„åŠ è½½ã€‚
    """
    
    def __init__(self, data_dir: Union[str, List[str]], group_by_k: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é›†ã€‚
        
        Args:
            data_dir (str): åŒ…å«å¤„ç†åçš„ .pt æ•°æ®æ–‡ä»¶çš„ç›®å½•ã€‚
            group_by_k (bool): æ˜¯å¦æŒ‰åˆ†åŒºæ•° k å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç»„ã€‚
        """
        # æ”¯æŒå•ç›®å½•æˆ–å¤šç›®å½•æ··åˆ
        if isinstance(data_dir, (list, tuple)):
            self.data_dirs = [Path(p) for p in data_dir]
        else:
            self.data_dirs = [Path(data_dir)]
        self.group_by_k = group_by_k
        self.optimizer = OptimizedProcessor()
        
        # åŠ è½½æ‰€æœ‰æ ·æœ¬
        self.samples = self._load_all_samples()
        
        if self.group_by_k:
            # å¦‚æœå¯ç”¨åˆ†ç»„ï¼Œåˆ™å°†æ ·æœ¬æŒ‰ k å€¼å­˜å…¥å­—å…¸
            self.samples_by_k = self._group_samples_by_k()
            self.k_values = sorted(list(self.samples_by_k.keys()))
            print(f"æ•°æ®å·²æŒ‰kå€¼åˆ†ç»„: {[f'k={k}:{len(samples)}ä¸ªæ ·æœ¬' for k, samples in self.samples_by_k.items()]}")
        else:
            print(f"å·²åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def _load_all_samples(self) -> List:
        """ä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰æ•°æ®æ ·æœ¬ã€‚"""
        all_samples = []
        # æŸ¥æ‰¾æ‰€æœ‰åä¸º chunk_*.pt çš„æ–‡ä»¶ï¼ˆæ”¯æŒå¤šä¸ªç›®å½•ï¼‰
        chunk_files: List[Path] = []
        for d in self.data_dirs:
            chunk_files.extend(sorted(d.glob('chunk_*.pt')))
        
        for chunk_file in chunk_files:
            try:
                # ä½¿ç”¨ä¼˜åŒ–è¿‡çš„åŠ è½½å™¨æ‰¹é‡åŠ è½½æ–‡ä»¶ä¸­çš„æ ·æœ¬
                samples = self.optimizer.load_batch_optimized(str(chunk_file), format='pt')
                all_samples.extend(samples)
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {chunk_file}: {e}")
                continue
                
        return all_samples
    
    def _group_samples_by_k(self) -> Dict[int, List]:
        """æ ¹æ®æ ·æœ¬è‡ªèº«çš„ 'k' å±æ€§å°†å®ƒä»¬åˆ†ç»„ã€‚"""
        samples_by_k = defaultdict(list)
        
        for sample in self.samples:
            # PyG Data å¯¹è±¡ä¸­åº”åŒ…å« k å±æ€§
            k = sample.k
            samples_by_k[k].append(sample)
        
        return dict(samples_by_k)
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†çš„é•¿åº¦ã€‚"""
        if self.group_by_k:
            # åœ¨åˆ†ç»„æ¨¡å¼ä¸‹ï¼Œæ•°æ®é›†çš„é•¿åº¦å®šä¹‰ä¸ºæ ·æœ¬æ•°æœ€å°‘çš„é‚£ä¸ªç»„çš„é•¿åº¦ã€‚
            # è¿™æ ·å¯ä»¥ç¡®ä¿åœ¨æ¯ä¸ª epoch ä¸­ï¼Œæ¯ä¸ª k å€¼åˆ†ç»„éƒ½èƒ½è¢«å®Œæ•´åœ°éå†è‡³å°‘ä¸€æ¬¡ã€‚
            if not self.samples_by_k:
                return 0
            return min(len(samples) for samples in self.samples_by_k.values())
        else:
            # åœ¨éåˆ†ç»„æ¨¡å¼ä¸‹ï¼Œé•¿åº¦å°±æ˜¯æ€»æ ·æœ¬æ•°ã€‚
            return len(self.samples)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ•°æ®é¡¹ã€‚
        
        Args:
            idx (int): ç´¢å¼•ã€‚
        
        Returns:
            - group_by_k=True: è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ¥è‡ªä¸åŒçš„ k å€¼åˆ†ç»„ã€‚
            - group_by_k=False: è¿”å›å•ä¸ª PyG Data æ ·æœ¬ã€‚
        """
        if self.group_by_k:
            # åœ¨åˆ†ç»„æ¨¡å¼ä¸‹ï¼Œä¸ºæ¯ä¸ª k å€¼åˆ†ç»„éƒ½å–å‡ºä¸€ä¸ªæ ·æœ¬ï¼Œç»„æˆä¸€ä¸ª "è¶…çº§" æ ·æœ¬æ‰¹æ¬¡
            batch_samples = []
            for k in self.k_values:
                k_samples = self.samples_by_k[k]
                # ä½¿ç”¨æ¨¡è¿ç®—ç¡®ä¿ç´¢å¼•ä¸ä¼šè¶Šç•Œï¼Œå®ç°å¾ªç¯é‡‡æ ·
                sample_idx = idx % len(k_samples)
                batch_samples.append(k_samples[sample_idx])
            return batch_samples
        else:
            # åœ¨éåˆ†ç»„æ¨¡å¼ä¸‹ï¼Œç›´æ¥è¿”å›å¯¹åº”ç´¢å¼•çš„æ ·æœ¬
            return self.samples[idx]


def collate_fn(batch):
    """
    è‡ªå®šä¹‰çš„ collate å‡½æ•°ï¼Œç”¨äºå°† `__getitem__` è¿”å›çš„æ•°æ®é¡¹æ‰“åŒ…æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚
    
    Args:
        batch: ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ `__getitem__` çš„è¿”å›å€¼ã€‚
    
    Returns:
        - group_by_k=True: è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸º 'k_2', 'k_3' ç­‰ï¼Œå€¼ä¸ºå¯¹åº” k å€¼çš„æ ·æœ¬æ‰¹æ¬¡ (Batch å¯¹è±¡)ã€‚
        - group_by_k=False: è¿”å›ä¸€ä¸ªæ ‡å‡†çš„ PyG Batch å¯¹è±¡ã€‚
    """
    if isinstance(batch[0], list):
        # æ­¤æƒ…å†µå¯¹åº” group_by_k=Trueã€‚
        # batch çš„ç»“æ„æ˜¯ [[k2_sample_0, k3_sample_0, ...], [k2_sample_1, k3_sample_1, ...], ...]
        k_batches = {}
        num_k_groups = len(batch[0])
        
        # éå†æ¯ä¸ª k å€¼åˆ†ç»„
        for k_idx in range(num_k_groups):
            # æå–å‡ºæ‰€æœ‰å±äºå½“å‰ k å€¼çš„æ ·æœ¬
            k_samples = [item[k_idx] for item in batch]
            k_value = k_samples[0].k # ä»ç¬¬ä¸€ä¸ªæ ·æœ¬è·å–kå€¼
            # ä½¿ç”¨ PyG çš„ Batch.from_data_list å°†å®ƒä»¬æ‰“åŒ…æˆä¸€ä¸ªç‹¬ç«‹çš„æ‰¹æ¬¡
            k_batches[f'k_{k_value}'] = Batch.from_data_list(k_samples)
            
        return k_batches
    else:
        # æ­¤æƒ…å†µå¯¹åº” group_by_k=Falseï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†æ–¹æ³•æ‰“åŒ…ã€‚
        return Batch.from_data_list(batch)


def create_dataloader(data_dir: str, 
                     batch_size: int = 32,
                     group_by_k: bool = True,
                     shuffle: bool = True,
                     num_workers: int = 4) -> DataLoader:
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªç”¨äºè®­ç»ƒçš„ DataLoaderã€‚
    
    Args:
        data_dir (str): æ•°æ®ç›®å½•ã€‚
        batch_size (int): æ‰¹æ¬¡å¤§å°ã€‚
        group_by_k (bool): æ˜¯å¦æŒ‰ k å€¼å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç»„ã€‚
        shuffle (bool): æ˜¯å¦åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶æ‰“ä¹±æ•°æ®ã€‚
        num_workers (int): ç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡ã€‚
    
    Returns:
        ä¸€ä¸ªé…ç½®å¥½çš„ torch.utils.data.DataLoader å¯¹è±¡ã€‚
    """
    dataset = OPFDataset(data_dir, group_by_k=group_by_k)
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰çš„ collate å‡½æ•°
        pin_memory=torch.cuda.is_available()  # å¦‚æœä½¿ç”¨ GPUï¼Œåˆ™å¯ç”¨ pin_memory ä»¥åŠ é€Ÿæ•°æ®ä¼ è¾“
    )


if __name__ == "__main__":
    # --- æ•°æ®åŠ è½½å™¨æµ‹è¯• ---
    print("=== æ•°æ®åŠ è½½å™¨æµ‹è¯• ===")
    
    # å‡è®¾æ•°æ®å·²ç»é€šè¿‡ batch_process.py å¤„ç†å¹¶å­˜æ”¾åœ¨æ­¤ç›®å½•
    data_dir = "/home/zhangyao/renjiashen/workspace/gnn_data/processed"
    
    if Path(data_dir).exists():
        # --- æµ‹è¯•åˆ†ç»„åŠ è½½ (group_by_k=True) ---
        print("\nğŸ” æµ‹è¯•æŒ‰ k å€¼åˆ†ç»„åŠ è½½:")
        loader = create_dataloader(data_dir, batch_size=2, group_by_k=True)
        
        # è¿­ä»£åŠ è½½å™¨å¹¶æ‰“å°å‰å‡ ä¸ªæ‰¹æ¬¡çš„ä¿¡æ¯ä»¥ä¾›æ£€æŸ¥
        for i, batch in enumerate(loader):
            print(f"æ‰¹æ¬¡ {i}:")
            # æ­¤æ—¶çš„ batch æ˜¯ä¸€ä¸ªå­—å…¸
            for k_name, k_batch in batch.items():
                print(f"  {k_name}: {k_batch.num_graphs} ä¸ªå›¾, {k_batch.num_nodes} ä¸ªèŠ‚ç‚¹, {k_batch.num_edges} æ¡è¾¹")
            if i >= 2:  # åªæŸ¥çœ‹å‰3ä¸ªæ‰¹æ¬¡
                break
                
        print(f"\næ•°æ®åŠ è½½å™¨ (åˆ†ç»„æ¨¡å¼) å·²å°±ç»ª: æ¯ä¸ª epoch æœ‰ {len(loader)} ä¸ªæ‰¹æ¬¡")
        
    else:
        print(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆè¿è¡Œ batch_process.py æ¥ç”Ÿæˆå¤„ç†å¥½çš„æ•°æ®ã€‚")
